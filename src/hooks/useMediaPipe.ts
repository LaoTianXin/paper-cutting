import React from "react";
import { Hands, HAND_CONNECTIONS, type Results } from "@mediapipe/hands";
import { Pose, POSE_CONNECTIONS, type Results as PoseResults } from "@mediapipe/pose";
import { Camera } from "@mediapipe/camera_utils";
import { drawConnectors, drawLandmarks } from "@mediapipe/drawing_utils";
import { initializePose, calculateBodyRect, type BodyRect } from "../poseDetection";
import { CaptureState } from "../types/capture";
import { recognizeOKGesture } from "../utils/gestureRecognition";

// ç›®æ ‡æ˜¾ç¤ºæ¯”ä¾‹ 10:16ï¼ˆä¸ CameraFeed ä¿æŒä¸€è‡´ï¼‰
const TARGET_ASPECT_RATIO = 10 / 16;

/**
 * è®¡ç®— 10:16 è£å‰ªåŒºåŸŸçš„è¾¹ç•Œï¼ˆå½’ä¸€åŒ–åæ ‡ 0-1ï¼‰
 * ç›¸æœºæ•è·çš„æ˜¯ 4:3 (640x480)ï¼Œæ˜¾ç¤ºçš„æ˜¯ä¸­é—´çš„ 10:16 åŒºåŸŸ
 */
function getCropBounds(sourceWidth: number, sourceHeight: number) {
  const sourceAspect = sourceWidth / sourceHeight;
  
  let cropX = 0, cropY = 0, cropWidth = 1, cropHeight = 1;
  
  if (sourceAspect > TARGET_ASPECT_RATIO) {
    // Source is wider (4:3 > 10:16), crop the width
    const visibleWidthRatio = (sourceHeight * TARGET_ASPECT_RATIO) / sourceWidth;
    cropWidth = visibleWidthRatio;
    cropX = (1 - visibleWidthRatio) / 2;
  } else {
    // Source is taller, crop the height
    const visibleHeightRatio = (sourceWidth / TARGET_ASPECT_RATIO) / sourceHeight;
    cropHeight = visibleHeightRatio;
    cropY = (1 - visibleHeightRatio) / 2;
  }
  
  return { cropX, cropY, cropWidth, cropHeight };
}

/**
 * æ£€æŸ¥å½’ä¸€åŒ–åæ ‡ç‚¹æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…
 */
function isPointInCropArea(x: number, y: number, bounds: ReturnType<typeof getCropBounds>): boolean {
  return x >= bounds.cropX && 
         x <= bounds.cropX + bounds.cropWidth &&
         y >= bounds.cropY && 
         y <= bounds.cropY + bounds.cropHeight;
}

/**
 * æ£€æŸ¥æ‰‹åŠ¿å…³é”®ç‚¹æ˜¯å¦å¤§éƒ¨åˆ†åœ¨è£å‰ªåŒºåŸŸå†…
 */
function isHandInCropArea(
  landmarks: { x: number; y: number }[],
  sourceWidth: number,
  sourceHeight: number
): boolean {
  const bounds = getCropBounds(sourceWidth, sourceHeight);
  // æ£€æŸ¥æ‰‹æŒä¸­å¿ƒç‚¹ï¼ˆå…³é”®ç‚¹0æ˜¯æ‰‹è…•ï¼‰
  const wrist = landmarks[0];
  const middleFinger = landmarks[9]; // ä¸­æŒ‡æ ¹éƒ¨
  const centerX = (wrist.x + middleFinger.x) / 2;
  const centerY = (wrist.y + middleFinger.y) / 2;
  
  return isPointInCropArea(centerX, centerY, bounds);
}

/**
 * æ£€æŸ¥äººä½“æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…ï¼ˆåŸºäº pose landmarksï¼‰
 */
function isBodyInCropArea(
  landmarks: { x: number; y: number; visibility?: number }[],
  sourceWidth: number,
  sourceHeight: number
): boolean {
  const bounds = getCropBounds(sourceWidth, sourceHeight);
  
  // æ£€æŸ¥å…³é”®èº«ä½“éƒ¨ä½æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…
  // ä½¿ç”¨èº¯å¹²ä¸­å¿ƒï¼ˆå·¦å³è‚©è†€å’Œå·¦å³é«‹éƒ¨çš„ä¸­å¿ƒï¼‰
  const leftShoulder = landmarks[11];
  const rightShoulder = landmarks[12];
  const leftHip = landmarks[23];
  const rightHip = landmarks[24];
  
  // æ£€æŸ¥å¯è§æ€§
  const visibleParts = [leftShoulder, rightShoulder, leftHip, rightHip].filter(
    lm => (lm?.visibility ?? 0) > 0.5
  );
  
  if (visibleParts.length < 2) return false;
  
  // è®¡ç®—èº¯å¹²ä¸­å¿ƒ
  const centerX = visibleParts.reduce((sum, lm) => sum + lm.x, 0) / visibleParts.length;
  const centerY = visibleParts.reduce((sum, lm) => sum + lm.y, 0) / visibleParts.length;
  
  return isPointInCropArea(centerX, centerY, bounds);
}

interface UseMediaPipeProps {
  onCapture?: (canvas: HTMLCanvasElement) => void;
}

export function useMediaPipe({ onCapture }: UseMediaPipeProps = {}) {
  const videoRef = React.useRef<HTMLVideoElement>(null);
  const canvasRef = React.useRef<HTMLCanvasElement>(null);
  
  const [state, setState] = React.useState<CaptureState>(CaptureState.IDLE);
  const [isLoading, setIsLoading] = React.useState<boolean>(true);
  const [error, setError] = React.useState<string | null>(null);
  const [statusMessage, setStatusMessage] = React.useState<string>("");
  const [countdown, setCountdown] = React.useState<number>(5);

  const stateRef = React.useRef(state);
  const countdownRef = React.useRef(countdown);
  const statusMessageRef = React.useRef(statusMessage);

  const bodyDetectionStartTime = React.useRef<number | null>(null);
  const lastBodyDetectedTime = React.useRef<number | null>(null);
  const gestureDetectionStartTime = React.useRef<number | null>(null);
  const gestureLastLostTime = React.useRef<number | null>(null);
  const lastBodyRectRef = React.useRef<BodyRect | null>(null);
  const lastPoseLandmarksRef = React.useRef<PoseResults["poseLandmarks"] | null>(null);
  const frameCountRef = React.useRef<number>(0);

  const poseRef = React.useRef<Pose | null>(null);
  const handsRef = React.useRef<Hands | null>(null);
  const cameraRef = React.useRef<Camera | null>(null);
  
  // ä¿å­˜æœ€æ–°çš„ç»“æœç”¨äºç»˜åˆ¶
  const latestPoseResultsRef = React.useRef<PoseResults | null>(null);
  const latestHandsResultsRef = React.useRef<Results | null>(null);
  const pendingDrawRef = React.useRef<boolean>(false);
  const frozenFrameRef = React.useRef<string | null>(null);

  React.useEffect(() => {
    stateRef.current = state;
    countdownRef.current = countdown;
    statusMessageRef.current = statusMessage;
  }, [state, countdown, statusMessage]);

  // ç»Ÿä¸€çš„ç»˜åˆ¶å‡½æ•°ï¼Œåœ¨ requestAnimationFrame ä¸­è°ƒç”¨
  const draw = React.useCallback(() => {
    const canvas = canvasRef.current;
    const video = videoRef.current;
    if (!canvas || !video) {
      pendingDrawRef.current = false;
      return;
    }

    const ctx = canvas.getContext("2d");
    if (!ctx) {
      pendingDrawRef.current = false;
      return;
    }

    const currentState = stateRef.current;
    const poseResults = latestPoseResultsRef.current;
    const handsResults = latestHandsResultsRef.current;

    // æ¸…ç©ºç”»å¸ƒå¹¶ç»˜åˆ¶è§†é¢‘å¸§
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // å§‹ç»ˆä½¿ç”¨ video å…ƒç´ ä½œä¸ºç»˜åˆ¶æºï¼Œä¿æŒä¸€è‡´æ€§ï¼Œé¿å…åˆ‡æ¢æ—¶é—ªçƒ
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // ç»˜åˆ¶ pose landmarks å’Œå…¨èº«æ¡†ï¼ˆåœ¨éæ‰‹åŠ¿æ£€æµ‹çŠ¶æ€ï¼Œä¸”äººä½“åœ¨è£å‰ªåŒºåŸŸå†…ï¼‰
    if (poseResults && poseResults.poseLandmarks && (
      currentState === CaptureState.IDLE ||
      currentState === CaptureState.DETECTING_BODY ||
      currentState === CaptureState.BODY_DETECTED ||
      currentState === CaptureState.COUNTDOWN ||
      currentState === CaptureState.COMPLETED
    )) {
      // åªæœ‰å½“äººä½“åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…æ—¶æ‰ç»˜åˆ¶
      const bodyInCropArea = isBodyInCropArea(poseResults.poseLandmarks, 640, 480);
      const rect = calculateBodyRect(poseResults.poseLandmarks, canvas.width, canvas.height);
      if (rect && bodyInCropArea) {
        if (
          currentState === CaptureState.IDLE ||
          currentState === CaptureState.DETECTING_BODY ||
          currentState === CaptureState.BODY_DETECTED
        ) {
          drawConnectors(ctx, poseResults.poseLandmarks, POSE_CONNECTIONS, { color: "#00FF00", lineWidth: 2 });
          drawLandmarks(ctx, poseResults.poseLandmarks, { color: "#FF0000", radius: 3 });
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 4;
          ctx.strokeRect(rect.x, rect.y, rect.width, rect.height);
          ctx.fillStyle = "#00FF00";
          ctx.font = "bold 12px Arial";
          ctx.fillText("å…¨èº«å·²æ£€æµ‹", rect.x, rect.y - 10);
        } else if (currentState === CaptureState.COUNTDOWN) {
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 3;
          ctx.strokeRect(rect.x, rect.y, rect.width, rect.height);
        } else if (currentState === CaptureState.COMPLETED) {
          ctx.strokeStyle = "rgba(0, 255, 0, 0.3)";
          ctx.lineWidth = 2;
          ctx.strokeRect(rect.x, rect.y, rect.width, rect.height);
        }
      }
    }

    // ç»˜åˆ¶å…¨èº«æ¡†ï¼ˆåœ¨æ‰‹åŠ¿æ£€æµ‹çŠ¶æ€ï¼Œä½¿ç”¨ç¼“å­˜çš„æ¡†ï¼‰
    if ((currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED) && lastBodyRectRef.current) {
      ctx.strokeStyle = "rgba(0, 255, 0, 0.8)";
      ctx.lineWidth = 3;
      ctx.strokeRect(lastBodyRectRef.current.x, lastBodyRectRef.current.y, lastBodyRectRef.current.width, lastBodyRectRef.current.height);
    }

    // ç»˜åˆ¶æ‰‹éƒ¨ landmarks å’Œ OK æ‰‹åŠ¿ï¼ˆåœ¨æ‰‹åŠ¿æ£€æµ‹çŠ¶æ€ï¼Œä¸”æ‰‹åœ¨è£å‰ªåŒºåŸŸå†…ï¼‰
    if (handsResults && handsResults.multiHandLandmarks && handsResults.multiHandLandmarks.length > 0 &&
        (currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED)) {
      for (const landmarks of handsResults.multiHandLandmarks) {
        // åªç»˜åˆ¶åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…çš„æ‰‹åŠ¿
        if (!isHandInCropArea(landmarks, 640, 480)) {
          continue;
        }
        drawConnectors(ctx, landmarks, HAND_CONNECTIONS, { color: "#00FF00", lineWidth: 3 });
        drawLandmarks(ctx, landmarks, { color: "#FF0000", lineWidth: 1, radius: 4 });

        const gestureResult = recognizeOKGesture(landmarks);
        if (gestureResult.isOK) {
          ctx.font = "bold 24px Arial";
          ctx.fillStyle = "#00FF00";
          ctx.strokeStyle = "#000000";
          ctx.lineWidth = 2;
          const text = "OK ğŸ‘Œ";
          const textWidth = ctx.measureText(text).width;
          ctx.strokeText(text, (canvas.width - textWidth) / 2, 80);
          ctx.fillText(text, (canvas.width - textWidth) / 2, 80);
        }
      }
    }

    // ç»˜åˆ¶å€’è®¡æ—¶
    if (currentState === CaptureState.COUNTDOWN) {
      ctx.fillStyle = "#FFD700";
      ctx.strokeStyle = "#000000";
      ctx.lineWidth = 3;
      ctx.font = "bold 60px Arial";
      const text = countdownRef.current.toString();
      const textWidth = ctx.measureText(text).width;
      ctx.strokeText(text, (canvas.width - textWidth) / 2, canvas.height / 2);
      ctx.fillText(text, (canvas.width - textWidth) / 2, canvas.height / 2);
    }

    // ç»˜åˆ¶çŠ¶æ€æ¶ˆæ¯
    const currentStatusMessage = statusMessageRef.current;
    if (currentStatusMessage && currentState !== CaptureState.COUNTDOWN) {
      ctx.fillStyle = "#FFFFFF";
      ctx.strokeStyle = "#000000";
      ctx.lineWidth = 2;
      ctx.font = "bold 16px Arial";
      const textWidth = ctx.measureText(currentStatusMessage).width;
      ctx.strokeText(currentStatusMessage, (canvas.width - textWidth) / 2, canvas.height - 50);
      ctx.fillText(currentStatusMessage, (canvas.width - textWidth) / 2, canvas.height - 50);
    }

    pendingDrawRef.current = false;
  }, []);

  // è¯·æ±‚ç»˜åˆ¶
  const requestDraw = React.useCallback(() => {
    if (!pendingDrawRef.current) {
      pendingDrawRef.current = true;
      requestAnimationFrame(draw);
    }
  }, [draw]);

  const onPoseResults = React.useCallback((results: PoseResults) => {
    const currentTime = Date.now();
    const currentState = stateRef.current;
    const canvas = canvasRef.current;
    if (!canvas) return;

    // ä¿å­˜æœ€æ–°çš„ç»“æœ
    latestPoseResultsRef.current = results;

    // å¤„ç†å…¨èº«æ£€æµ‹é€»è¾‘
    if (results.poseLandmarks) {
      // æ£€æŸ¥äººä½“æ˜¯å¦åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…
      const isInCropArea = isBodyInCropArea(results.poseLandmarks, 640, 480);
      const rect = calculateBodyRect(results.poseLandmarks, canvas.width, canvas.height);
      
      if (rect && isInCropArea) {
        lastBodyDetectedTime.current = currentTime;
        lastBodyRectRef.current = rect;
        lastPoseLandmarksRef.current = results.poseLandmarks;

        if (currentState !== CaptureState.COMPLETED) {
          if (currentState === CaptureState.IDLE) {
            setState(CaptureState.DETECTING_BODY);
            bodyDetectionStartTime.current = currentTime;
            setStatusMessage("æ­£åœ¨æ£€æµ‹å…¨èº«...");
          } else if (currentState === CaptureState.DETECTING_BODY) {
            if (bodyDetectionStartTime.current && currentTime - bodyDetectionStartTime.current >= 1000) {
              setState(CaptureState.DETECTING_GESTURE);
              setStatusMessage("âœ“ å·²è¯†åˆ«åˆ°å…¨èº«ï¼Œè¯·åšå‡ºOKæ‰‹åŠ¿");
            }
          }
        }
      } else if (!isInCropArea && currentState === CaptureState.DETECTING_BODY) {
        // äººä½“ä¸åœ¨è£å‰ªåŒºåŸŸå†…ï¼Œæç¤ºç”¨æˆ·
        setStatusMessage("è¯·ç«™åˆ°ç”»é¢ä¸­å¤®");
      }
    } else {
      // å…¨èº«ä¸¢å¤±å¤„ç†
      if (currentState === CaptureState.DETECTING_BODY) {
        if (lastBodyDetectedTime.current && currentTime - lastBodyDetectedTime.current >= 1000) {
          setState(CaptureState.IDLE);
          bodyDetectionStartTime.current = null;
          lastBodyDetectedTime.current = null;
          lastPoseLandmarksRef.current = null;
          setStatusMessage("âŒ æœªè¯†åˆ«åˆ°å…¨èº«ï¼Œè¯·é‡æ–°ç«™ä½");
          setTimeout(() => setStatusMessage(""), 1500);
        }
      }
    }

    // è¯·æ±‚ç»˜åˆ¶
    requestDraw();
  }, [requestDraw]);

  const onHandsResults = React.useCallback((results: Results) => {
    const currentTime = Date.now();
    const currentState = stateRef.current;
    const video = videoRef.current;
    if (!video) return;

    // ä¿å­˜æœ€æ–°çš„ç»“æœ
    latestHandsResultsRef.current = results;

    if (currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED) {
      // æ¯5å¸§æ£€æµ‹ä¸€æ¬¡poseï¼Œå‡å°‘æ›´æ–°é¢‘ç‡ï¼Œé¿å…é—ªåŠ¨
      frameCountRef.current++;
      if (frameCountRef.current % 5 === 0 && poseRef.current && video) {
        try {
          poseRef.current.send({ image: video }).catch(() => {});
        } catch (err) {}
      }

      // æ£€æŸ¥å…¨èº«æ˜¯å¦ä¸¢å¤±
      if (lastBodyDetectedTime.current && currentTime - lastBodyDetectedTime.current >= 1000) {
        setState(CaptureState.IDLE);
        bodyDetectionStartTime.current = null;
        lastBodyDetectedTime.current = null;
        gestureDetectionStartTime.current = null;
        frameCountRef.current = 0;
        setStatusMessage("âŒ å…¨èº«ä¸¢å¤±ï¼Œé‡æ–°æ£€æµ‹");
        setTimeout(() => setStatusMessage(""), 1500);
        requestDraw();
        return;
      }

      // æ£€æµ‹ OK æ‰‹åŠ¿ï¼ˆåªæ£€æµ‹åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…çš„æ‰‹åŠ¿ï¼‰
      let isOKDetected = false;
      if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
        for (const landmarks of results.multiHandLandmarks) {
          // å…ˆæ£€æŸ¥æ‰‹åŠ¿æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…
          if (!isHandInCropArea(landmarks, 640, 480)) {
            continue;
          }
          const gestureResult = recognizeOKGesture(landmarks);
          if (gestureResult.isOK) {
            isOKDetected = true;
            break;
          }
        }
      }

      // å¤„ç†æ‰‹åŠ¿çŠ¶æ€
      if (isOKDetected) {
        gestureLastLostTime.current = null;
        
        if (currentState === CaptureState.DETECTING_GESTURE) {
          gestureDetectionStartTime.current = currentTime;
          setState(CaptureState.GESTURE_DETECTED);
        } else if (currentState === CaptureState.GESTURE_DETECTED && gestureDetectionStartTime.current) {
          const elapsed = currentTime - gestureDetectionStartTime.current;
          const remaining = Math.ceil((3000 - elapsed) / 1000);

          if (elapsed >= 3000) {
            setState(CaptureState.COUNTDOWN);
            setCountdown(5);
            setStatusMessage("å‡†å¤‡æ‹ç…§ï¼");
          } else {
            setStatusMessage(`ä¿æŒOKæ‰‹åŠ¿ ${remaining}s`);
          }
        }
      } else {
        // æ‰‹åŠ¿æœªæ£€æµ‹åˆ°
        if (currentState === CaptureState.GESTURE_DETECTED) {
          // åªæœ‰åœ¨æ‰‹åŠ¿æŒç»­ä¸¢å¤±300msåæ‰åˆ‡æ¢çŠ¶æ€ï¼Œé¿å…é—ªåŠ¨
          if (gestureLastLostTime.current === null) {
            gestureLastLostTime.current = currentTime;
          } else if (currentTime - gestureLastLostTime.current >= 300) {
            setState(CaptureState.DETECTING_GESTURE);
            gestureDetectionStartTime.current = null;
            gestureLastLostTime.current = null;
            setStatusMessage("è¯·åšå‡ºOKæ‰‹åŠ¿");
          }
        }
      }

      // è¯·æ±‚ç»˜åˆ¶
      requestDraw();
    }
  }, [requestDraw]);

  React.useEffect(() => {
    if (state === CaptureState.COUNTDOWN) {
      if (countdown > 0) {
        const timer = setTimeout(() => setCountdown(countdown - 1), 1000);
        return () => clearTimeout(timer);
      } else {
        // å€’è®¡æ—¶ç»“æŸï¼Œæ•è·å½“å‰å¸§ä½œä¸ºfrozen frame
        // ä» video å…ƒç´ æ•è·ï¼Œè€Œä¸æ˜¯ canvasï¼Œè¿™æ ·ä¸ä¼šåŒ…å«çº¿æ¡†å’Œå€’è®¡æ—¶
        const video = videoRef.current;
        const canvas = canvasRef.current;
        if (video && canvas) {
          try {
            // åˆ›å»ºä¸´æ—¶ canvas ç”¨äºæ•è·å’Œè£å‰ª
            const tempCanvas = document.createElement('canvas');
            const sourceWidth = video.videoWidth || 640;
            const sourceHeight = video.videoHeight || 480;
            
            // è®¡ç®— 9:16 è£å‰ªåŒºåŸŸï¼ˆä¸ CameraFeed ä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
            const targetAspect = 9 / 16;
            const sourceAspect = sourceWidth / sourceHeight;
            
            let cropWidth, cropHeight, cropX, cropY;
            if (sourceAspect > targetAspect) {
              // Source is wider, crop the width
              cropHeight = sourceHeight;
              cropWidth = sourceHeight * targetAspect;
              cropX = (sourceWidth - cropWidth) / 2;
              cropY = 0;
            } else {
              // Source is taller, crop the height
              cropWidth = sourceWidth;
              cropHeight = sourceWidth / targetAspect;
              cropX = 0;
              cropY = (sourceHeight - cropHeight) / 2;
            }
            
            // è®¾ç½®ä¸´æ—¶ canvas ä¸ºè£å‰ªåçš„å°ºå¯¸
            tempCanvas.width = cropWidth;
            tempCanvas.height = cropHeight;
            
            const tempCtx = tempCanvas.getContext('2d');
            if (tempCtx) {
              // ä» video ç»˜åˆ¶è£å‰ªåçš„åŒºåŸŸåˆ°ä¸´æ—¶ canvas
              tempCtx.drawImage(
                video,
                cropX, cropY, cropWidth, cropHeight,
                0, 0, cropWidth, cropHeight
              );
              
              // è½¬æ¢ä¸º data URL
              const frozenFrameUrl = tempCanvas.toDataURL('image/png');
              frozenFrameRef.current = frozenFrameUrl;
              console.log('âœ… Frozen frame captured at countdown end (clean, 9:16 cropped)');
            }
          } catch (err) {
            console.error('âŒ Failed to capture frozen frame:', err);
          }
        }
        // ç«‹å³è¿›å…¥æ‹ç…§ä¸­çŠ¶æ€æ˜¾ç¤ºå¿«é—¨æ•ˆæœ
        setState(CaptureState.CAPTURING);
      }
    }
  }, [state, countdown]);

  // Handle capturing state - show shutter effect then take photo
  React.useEffect(() => {
    if (state === CaptureState.CAPTURING) {
      const captureTimer = setTimeout(() => {
        setState(CaptureState.CAPTURE);
      }, 3000); // 3ç§’æ‹ç…§å»¶è¿Ÿï¼Œæ˜¾ç¤ºå¿«é—¨æ•ˆæœ
      return () => clearTimeout(captureTimer);
    }
  }, [state]);

  // Handle photo capture
  React.useEffect(() => {
    if (state === CaptureState.CAPTURE) {
      const video = videoRef.current;
      const canvas = canvasRef.current;
      if (video && canvas && lastBodyRectRef.current) {
        const tempCanvas = document.createElement("canvas");
        tempCanvas.width = video.videoWidth || 640;
        tempCanvas.height = video.videoHeight || 480;
        const tempCtx = tempCanvas.getContext("2d");
        if (tempCtx) {
          tempCtx.drawImage(video, 0, 0, tempCanvas.width, tempCanvas.height);
          const scaleX = tempCanvas.width / canvas.width;
          const scaleY = tempCanvas.height / canvas.height;
          const rect = lastBodyRectRef.current;
          const padding = 20;
          const x = Math.max(0, (rect.x - padding) * scaleX);
          const y = Math.max(0, (rect.y - padding) * scaleY);
          const width = Math.min(tempCanvas.width - x, (rect.width + padding * 2) * scaleX);
          const height = Math.min(tempCanvas.height - y, (rect.height + padding * 2) * scaleY);

          const resultCanvas = document.createElement("canvas");
          resultCanvas.width = width * 1.5;
          resultCanvas.height = height * 1.5;
          const resultCtx = resultCanvas.getContext("2d");
          if (resultCtx) {
            resultCtx.drawImage(tempCanvas, x, y, width, height, 0, 0, resultCanvas.width, resultCanvas.height);
            onCapture?.(resultCanvas);
          }
        }
        setState(CaptureState.COMPLETED);
        setStatusMessage("âœ“ æ‹ç…§å®Œæˆï¼");
      }
    }
  }, [state, onCapture]);

  // Track when video element is ready
  const [videoReady, setVideoReady] = React.useState(false);
  
  // Callback ref for video element
  const videoCallbackRef = React.useCallback((node: HTMLVideoElement | null) => {
    videoRef.current = node;
    if (node) {
      console.log('âœ… Video å…ƒç´ å·²é™„åŠ åˆ° DOM');
      setVideoReady(true);
    }
  }, []);

  // Initialization effect - runs when video is ready
  React.useEffect(() => {
    if (!videoReady) {
      console.log('ç­‰å¾… video å…ƒç´ ...');
      return;
    }

    let mounted = true;
    let camera: Camera | null = null;
    let hands: Hands | null = null;
    let pose: Pose | null = null;

    const initialize = async () => {
      try {
        console.log('=======å¼€å§‹åˆå§‹åŒ–=======');
        console.log('videoRef.current:', videoRef.current);
        console.log('canvasRef.current:', canvasRef.current);
        
        if (!videoRef.current || !canvasRef.current) {
          throw new Error('Video æˆ– Canvas å…ƒç´ æœªæ‰¾åˆ°');
        }

        const [poseInstance, handsInstance] = await Promise.all([
          initializePose(),
          (async () => {
            const h = new Hands({ locateFile: (file) => `/mediapipe/hands/${file}` });
            h.setOptions({ maxNumHands: 2, modelComplexity: 1, minDetectionConfidence: 0.7, minTrackingConfidence: 0.5 });
            return h;
          })(),
        ]);

        if (!mounted) return;
        pose = poseInstance;
        pose.onResults(onPoseResults);
        poseRef.current = pose;
        hands = handsInstance;
        hands.onResults(onHandsResults);
        handsRef.current = hands;

        console.log('åˆ›å»º Camera å®ä¾‹...');
        camera = new Camera(videoRef.current, {
          onFrame: async () => {
            if (!mounted || !videoRef.current) return;
            const currentState = stateRef.current;
            
            try {
              // åœ¨æ‰‹åŠ¿æ£€æµ‹é˜¶æ®µåŒæ—¶è¿è¡Œ pose å’Œ hands
              if (
                currentState === CaptureState.DETECTING_GESTURE ||
                currentState === CaptureState.GESTURE_DETECTED
              ) {
                // ä¸ç­‰å¾… pose å®Œæˆï¼Œè®©å®ƒåœ¨åå°è¿è¡Œ
                if (pose && mounted) {
                  pose.send({ image: videoRef.current }).catch(() => {});
                }
                // hands æ˜¯ä¸»è¦å¤„ç†ï¼Œç­‰å¾…å®ƒå®Œæˆ
                if (hands && mounted) {
                  await hands.send({ image: videoRef.current });
                }
              } else {
                // å…¶ä»–çŠ¶æ€åªè¿è¡Œ pose
                if (pose && mounted) {
                  await pose.send({ image: videoRef.current });
                }
              }
            } catch (err) {
              // å¿½ç•¥é”™è¯¯ï¼Œç»§ç»­ä¸‹ä¸€å¸§
            }
          },
          width: 640,
          height: 480,
        });

        console.log('æ­£åœ¨å¯åŠ¨æ‘„åƒå¤´...');
        await camera.start();
        console.log('âœ… æ‘„åƒå¤´å¯åŠ¨æˆåŠŸï¼');
        
        cameraRef.current = camera;
        
        // é¢„çƒ­ hands æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡ä½¿ç”¨æ—¶å¡é¡¿
        console.log('é¢„çƒ­ hands æ¨¡å‹...');
        if (videoRef.current && hands) {
          try {
            await hands.send({ image: videoRef.current });
            console.log('âœ… hands æ¨¡å‹é¢„çƒ­å®Œæˆ');
          } catch (err) {
            console.log('é¢„çƒ­å¤±è´¥ï¼Œä½†ä¸å½±å“ä½¿ç”¨');
          }
        }
        
        setIsLoading(false);
        setState(CaptureState.IDLE);
        setStatusMessage("ç«™åœ¨æ‘„åƒå¤´å‰å¼€å§‹æ£€æµ‹");
        console.log('âœ… åˆå§‹åŒ–å®Œæˆ');
      } catch (err) {
        console.error('âŒ åˆå§‹åŒ–å¤±è´¥:', err);
        if (mounted) {
          setError(`æ— æ³•å¯åŠ¨æ‘„åƒå¤´æˆ–åŠ è½½æ¨¡å‹: ${err instanceof Error ? err.message : String(err)}`);
          setIsLoading(false);
        }
      }
    };

    initialize();

    return () => {
      mounted = false;
      if (camera) camera.stop();
      setTimeout(() => {
        if (hands) hands.close();
        if (pose) pose.close();
      }, 100);
    };
  }, [videoReady, onHandsResults, onPoseResults]);

  
  const handleReset = () => {
    setState(CaptureState.IDLE);
    setStatusMessage("");
    setCountdown(5);
    bodyDetectionStartTime.current = null;
    lastBodyDetectedTime.current = null;
    gestureDetectionStartTime.current = null;
    gestureLastLostTime.current = null;
    lastBodyRectRef.current = null;
    lastPoseLandmarksRef.current = null;
    frameCountRef.current = 0;
    frozenFrameRef.current = null;
  };

  return {
    videoRef: videoCallbackRef,
    canvasRef,
    state,
    isLoading,
    error,
    statusMessage,
    countdown,
    handleReset,
    frozenFrame: frozenFrameRef.current,
  };
}

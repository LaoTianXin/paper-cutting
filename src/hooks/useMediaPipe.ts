import React from "react";
import { Hands, HAND_CONNECTIONS, type Results } from "@mediapipe/hands";
import { Pose, POSE_CONNECTIONS, type Results as PoseResults } from "@mediapipe/pose";
import { Camera } from "@mediapipe/camera_utils";
import { drawConnectors, drawLandmarks } from "@mediapipe/drawing_utils";
import { initializePose, calculateBodyRect, type BodyRect } from "../poseDetection";
import { CaptureState } from "../types/capture";
import { recognizeOKGesture } from "../utils/gestureRecognition";
import { useSettings } from "../contexts/SettingsContext";

// ç›®æ ‡æ˜¾ç¤ºæ¯”ä¾‹ 10:16ï¼ˆä¸ CameraFeed ä¿æŒä¸€è‡´ï¼‰
const TARGET_ASPECT_RATIO = 10 / 16;

/**
 * è®¡ç®— 10:16 è£å‰ªåŒºåŸŸçš„è¾¹ç•Œï¼ˆå½’ä¸€åŒ–åæ ‡ 0-1ï¼‰
 * ç›¸æœºæ•è·çš„æ˜¯ 4:3 (640x480)ï¼Œæ˜¾ç¤ºçš„æ˜¯ä¸­é—´çš„ 10:16 åŒºåŸŸ
 */
function getCropBounds(sourceWidth: number, sourceHeight: number) {
  const sourceAspect = sourceWidth / sourceHeight;
  
  let cropX = 0, cropY = 0, cropWidth = 1, cropHeight = 1;
  
  if (sourceAspect > TARGET_ASPECT_RATIO) {
    // Source is wider (4:3 > 10:16), crop the width
    const visibleWidthRatio = (sourceHeight * TARGET_ASPECT_RATIO) / sourceWidth;
    cropWidth = visibleWidthRatio;
    cropX = (1 - visibleWidthRatio) / 2;
  } else {
    // Source is taller, crop the height
    const visibleHeightRatio = (sourceWidth / TARGET_ASPECT_RATIO) / sourceHeight;
    cropHeight = visibleHeightRatio;
    cropY = (1 - visibleHeightRatio) / 2;
  }
  
  return { cropX, cropY, cropWidth, cropHeight };
}

/**
 * æ£€æŸ¥å½’ä¸€åŒ–åæ ‡ç‚¹æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…
 */
function isPointInCropArea(x: number, y: number, bounds: ReturnType<typeof getCropBounds>): boolean {
  return x >= bounds.cropX && 
         x <= bounds.cropX + bounds.cropWidth &&
         y >= bounds.cropY && 
         y <= bounds.cropY + bounds.cropHeight;
}

/**
 * æ£€æŸ¥æ‰‹åŠ¿å…³é”®ç‚¹æ˜¯å¦å¤§éƒ¨åˆ†åœ¨è£å‰ªåŒºåŸŸå†…
 */
function isHandInCropArea(
  landmarks: { x: number; y: number }[],
  sourceWidth: number,
  sourceHeight: number
): boolean {
  const bounds = getCropBounds(sourceWidth, sourceHeight);
  // æ£€æŸ¥æ‰‹æŒä¸­å¿ƒç‚¹ï¼ˆå…³é”®ç‚¹0æ˜¯æ‰‹è…•ï¼‰
  const wrist = landmarks[0];
  const middleFinger = landmarks[9]; // ä¸­æŒ‡æ ¹éƒ¨
  const centerX = (wrist.x + middleFinger.x) / 2;
  const centerY = (wrist.y + middleFinger.y) / 2;
  
  return isPointInCropArea(centerX, centerY, bounds);
}

/**
 * æ£€æŸ¥äººä½“æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…ï¼ˆåŸºäº pose landmarksï¼‰
 */
function isBodyInCropArea(
  landmarks: { x: number; y: number; visibility?: number }[],
  sourceWidth: number,
  sourceHeight: number
): boolean {
  const bounds = getCropBounds(sourceWidth, sourceHeight);
  
  // æ£€æŸ¥å…³é”®èº«ä½“éƒ¨ä½æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…
  // ä½¿ç”¨èº¯å¹²ä¸­å¿ƒï¼ˆå·¦å³è‚©è†€å’Œå·¦å³é«‹éƒ¨çš„ä¸­å¿ƒï¼‰
  const leftShoulder = landmarks[11];
  const rightShoulder = landmarks[12];
  const leftHip = landmarks[23];
  const rightHip = landmarks[24];
  
  // æ£€æŸ¥å¯è§æ€§
  const visibleParts = [leftShoulder, rightShoulder, leftHip, rightHip].filter(
    lm => (lm?.visibility ?? 0) > 0.5
  );
  
  if (visibleParts.length < 2) return false;
  
  // è®¡ç®—èº¯å¹²ä¸­å¿ƒ
  const centerX = visibleParts.reduce((sum, lm) => sum + lm.x, 0) / visibleParts.length;
  const centerY = visibleParts.reduce((sum, lm) => sum + lm.y, 0) / visibleParts.length;
  
  return isPointInCropArea(centerX, centerY, bounds);
}

interface UseMediaPipeProps {
  onCapture?: (canvas: HTMLCanvasElement) => void;
}

export function useMediaPipe({ onCapture }: UseMediaPipeProps = {}) {
  // Get settings from context
  const { settings, updateFps } = useSettings();
  const settingsRef = React.useRef(settings);
  
  const videoRef = React.useRef<HTMLVideoElement>(null);
  const canvasRef = React.useRef<HTMLCanvasElement>(null);
  
  const [state, setState] = React.useState<CaptureState>(CaptureState.IDLE);
  const [isLoading, setIsLoading] = React.useState<boolean>(true);
  const [error, setError] = React.useState<string | null>(null);
  const [statusMessage, setStatusMessage] = React.useState<string>("");
  const [countdown, setCountdown] = React.useState<number>(5);

  const stateRef = React.useRef(state);
  const countdownRef = React.useRef(countdown);
  const statusMessageRef = React.useRef(statusMessage);

  const bodyDetectionStartTime = React.useRef<number | null>(null);
  const lastBodyDetectedTime = React.useRef<number | null>(null);
  const gestureDetectionStartTime = React.useRef<number | null>(null);
  const gestureLastLostTime = React.useRef<number | null>(null);
  const lastBodyRectRef = React.useRef<BodyRect | null>(null);
  const lastPoseLandmarksRef = React.useRef<PoseResults["poseLandmarks"] | null>(null);
  const frameCountRef = React.useRef<number>(0);

  const poseRef = React.useRef<Pose | null>(null);
  const handsRef = React.useRef<Hands | null>(null);
  const cameraRef = React.useRef<Camera | null>(null);
  
  // ä¿å­˜å®é™…è§†é¢‘åˆ†è¾¨ç‡ï¼Œé¿å…ç¡¬ç¼–ç  (é«˜åˆ†è¾¨ç‡ 1280x960)
  const videoDimensionsRef = React.useRef<{ width: number; height: number }>({ width: 1280, height: 960 });
  
  // ä¿å­˜æœ€æ–°çš„ç»“æœç”¨äºç»˜åˆ¶
  const latestPoseResultsRef = React.useRef<PoseResults | null>(null);
  const latestHandsResultsRef = React.useRef<Results | null>(null);
  const pendingDrawRef = React.useRef<boolean>(false);
  const frozenFrameRef = React.useRef<string | null>(null);
  
  // FPS tracking
  const fpsFrameCountRef = React.useRef(0);
  const fpsLastTimeRef = React.useRef(performance.now());
  
  // Update settingsRef when settings change
  React.useEffect(() => {
    settingsRef.current = settings;
  }, [settings]);

  React.useEffect(() => {
    stateRef.current = state;
    countdownRef.current = countdown;
    statusMessageRef.current = statusMessage;
  }, [state, countdown, statusMessage]);

  // ç»Ÿä¸€çš„ç»˜åˆ¶å‡½æ•°ï¼Œåœ¨ requestAnimationFrame ä¸­è°ƒç”¨
  const draw = React.useCallback(() => {
    const canvas = canvasRef.current;
    const video = videoRef.current;
    if (!canvas || !video) {
      pendingDrawRef.current = false;
      return;
    }

    const ctx = canvas.getContext("2d");
    if (!ctx) {
      pendingDrawRef.current = false;
      return;
    }

    const currentState = stateRef.current;
    const poseResults = latestPoseResultsRef.current;
    const handsResults = latestHandsResultsRef.current;
    const debugMode = settingsRef.current.debugMode;

    // Calcalate scale based on 1280 width baseline
    const scale = canvas.width / 1280;

    // FPS calculation
    fpsFrameCountRef.current++;
    const now = performance.now();
    if (now - fpsLastTimeRef.current >= 1000) {
      const fps = Math.round(fpsFrameCountRef.current * 1000 / (now - fpsLastTimeRef.current));
      updateFps(fps);
      fpsFrameCountRef.current = 0;
      fpsLastTimeRef.current = now;
    }

    // æ¸…ç©ºç”»å¸ƒ
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // è®¡ç®— object-fit: cover çš„è£å‰ªå‚æ•°
    // ç¡®ä¿è§†é¢‘å¡«å……æ•´ä¸ªç”»å¸ƒä¸”ä¸å˜å½¢
    const vWidth = video.videoWidth;
    const vHeight = video.videoHeight;
    
    // å¦‚æœè§†é¢‘å°šæœªå‡†å¤‡å¥½ï¼Œç›´æ¥è¿”å›
    if (!vWidth || !vHeight) {
      pendingDrawRef.current = false;
      return;
    }
    
    const cWidth = canvas.width;
    const cHeight = canvas.height;
    
    // è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼šå–å®½æ¯”å’Œé«˜æ¯”çš„æœ€å¤§å€¼ï¼Œç¡®ä¿å¡«æ»¡ç”»å¸ƒ
    const scaleRatio = Math.max(cWidth / vWidth, cHeight / vHeight);
    
    // è®¡ç®—åœ¨åŸè§†é¢‘ä¸Šéœ€è¦æˆªå–çš„åŒºåŸŸ (source rect)
    const sWidth = cWidth / scaleRatio;
    const sHeight = cHeight / scaleRatio;
    
    // å±…ä¸­è£å‰ª
    const sx = (vWidth - sWidth) / 2;
    const sy = (vHeight - sHeight) / 2;
    
    // ç»˜åˆ¶è£å‰ªåçš„è§†é¢‘å¸§
    ctx.drawImage(video, sx, sy, sWidth, sHeight, 0, 0, cWidth, cHeight);

    // è®¡ç®—è°ƒè¯•ç»˜åˆ¶çš„å˜æ¢çŸ©é˜µ
    // MediaPipe è¿”å›çš„ landmarks æ˜¯å½’ä¸€åŒ–çš„ (0-1)ï¼Œç›¸å¯¹äºåŸå§‹è§†é¢‘å°ºå¯¸
    // æˆ‘ä»¬éœ€è¦å°†å…¶æ˜ å°„åˆ°è£å‰ªåçš„ç”»å¸ƒåæ ‡ç³»
    // CanvasX = (VideoX - sx) * (cWidth / sWidth)
    // VideoX = LandmarkX * vWidth
    // æ‰€ä»¥: CanvasX = LandmarkX * vWidth * (cWidth / sWidth) - sx * (cWidth / sWidth)
    // Scale = vWidth / sWidth
    // Translate = -sx * (cWidth / sWidth)
    
    // æ³¨æ„: sWidth = cWidth / scaleRatio => cWidth / sWidth = scaleRatio
    // æ‰€ä»¥ Scale = vWidth * scaleRatio / cWidth ??? 
    // ä¸ï¼Œç®€å•æ¨å¯¼:
    // æˆ‘ä»¬å°†è§†é¢‘åŒºåŸŸ [sx, sx+sWidth] æ˜ å°„åˆ°äº† [0, cWidth]
    // ç¼©æ”¾å› å­ k = cWidth / sWidth
    // å¹³ç§» = -sx * k
    
    const k = cWidth / sWidth;
    const transX = -sx * k;
    const transY = -sy * k;
    const scaleX = vWidth * k / cWidth; // drawLandmarks å†…éƒ¨ä¹˜ä»¥ canvas.widthï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ä½¿å¾— unit 1 å¯¹åº” vWidth * k
    const scaleY = vHeight * k / cHeight;
    
    // ç­‰ç­‰ï¼ŒdrawLandmarks ä½¿ç”¨ x * canvas.widthã€‚
    // æˆ‘ä»¬å¸Œæœ› x * canvas.width å˜æ¢åç­‰äº (x * vWidth - sx) * k
    // Transformed(x * cWidth) = x * cWidth * S + T
    // Target = x * vWidth * k - sx * k
    // æ‰€ä»¥ S = (vWidth * k) / cWidth
    // T = -sx * k
    
    // ä¿å­˜ä¸Šä¸‹æ–‡çŠ¶æ€
    ctx.save();
    ctx.translate(transX, transY);
    ctx.scale(scaleX, scaleY);

    // ç»˜åˆ¶ pose landmarks å’Œå…¨èº«æ¡†ï¼ˆä»…åœ¨ debugMode å¼€å¯æ—¶ï¼‰
    if (debugMode && poseResults && poseResults.poseLandmarks && (
      currentState === CaptureState.IDLE ||
      currentState === CaptureState.DETECTING_BODY ||
      currentState === CaptureState.BODY_DETECTED ||
      currentState === CaptureState.COUNTDOWN ||
      currentState === CaptureState.COMPLETED
    )) {
      // åªæœ‰å½“äººä½“åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…æ—¶æ‰ç»˜åˆ¶
      const { width: videoW, height: videoH } = videoDimensionsRef.current;
      const bodyInCropArea = isBodyInCropArea(poseResults.poseLandmarks, videoW, videoH);
      const rect = calculateBodyRect(poseResults.poseLandmarks, canvas.width, canvas.height); // è¿™é‡Œ calculateBodyRect ä½¿ç”¨äº† canvas.widthï¼Œä½†å› ä¸ºæˆ‘ä»¬ç¼©æ”¾äº† contextï¼Œè¿™é‡Œéœ€è¦æ³¨æ„
      // calculateBodyRect è¿”å›çš„æ˜¯åƒç´ å€¼ (åŸºäº canvas.width)ã€‚
      // æˆ‘ä»¬çš„ context å·²ç»ç¼©æ”¾äº†ã€‚å¦‚æœä¸è°ƒæ•´ï¼Œrect ä¹Ÿä¼šè¢«ç¼©æ”¾ã€‚
      // ä½†æ˜¯ rect æ˜¯åŸºäº landmarks * canvas.width è®¡ç®—çš„ã€‚
      // æˆ‘ä»¬çš„ context ç¼©æ”¾æ˜¯ä¸ºäº†è®© "landmarks * canvas.width" æ­£ç¡®æ˜ å°„ã€‚
      // æ‰€ä»¥ rect åº”è¯¥ä¹Ÿæ˜¯æ­£ç¡®çš„ã€‚
      
      if (rect && bodyInCropArea) {
        if (
          currentState === CaptureState.IDLE ||
          currentState === CaptureState.DETECTING_BODY ||
          currentState === CaptureState.BODY_DETECTED
        ) {
          // ç”±äº context ç¼©æ”¾äº†ï¼ŒlineWidth ä¹Ÿä¼šè¢«ç¼©æ”¾ã€‚
          // scale å˜é‡ (canvas.width / 1280) å·²ç»å¤„ç†äº†å±å¹•é€‚é…ã€‚
          // è¿™é‡Œçš„ scaleX/scaleY å¤„ç†äº† crop é€‚é… (zoom)ã€‚
          // çœ‹èµ·æ¥æ˜¯åˆç†çš„ã€‚
          drawConnectors(ctx, poseResults.poseLandmarks, POSE_CONNECTIONS, { color: "#00FF00", lineWidth: 4 * scale });
          drawLandmarks(ctx, poseResults.poseLandmarks, { color: "#FF0000", radius: 6 * scale });
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 8 * scale;
          ctx.strokeRect(rect.x, rect.y, rect.width, rect.height);
          ctx.fillStyle = "#00FF00";
          ctx.font = `bold ${Math.round(24 * scale)}px Arial`;
          // æ¢å¤æ–‡å­—ç»˜åˆ¶æ—¶çš„ scaleï¼Œé¿å…æ–‡å­—å˜å½¢ï¼ˆå¦‚æœ scaleX != scaleYï¼‰
          // è¿™é‡Œ scaleX = scaleY (å› ä¸º sWidth/sHeight = cWidth/cHeight = aspect ratio preserved)
          ctx.fillText("å…¨èº«å·²æ£€æµ‹", rect.x, rect.y - 20 * scale);
        } else if (currentState === CaptureState.COUNTDOWN) {
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 6 * scale;
          ctx.strokeRect(rect.x, rect.y, rect.width, rect.height);
        } else if (currentState === CaptureState.COMPLETED) {
          ctx.strokeStyle = "rgba(0, 255, 0, 0.3)";
          ctx.lineWidth = 4 * scale;
          ctx.strokeRect(rect.x, rect.y, rect.width, rect.height);
        }
      }
    }

    // ç»˜åˆ¶å…¨èº«æ¡†ï¼ˆåœ¨æ‰‹åŠ¿æ£€æµ‹çŠ¶æ€ï¼Œä½¿ç”¨ç¼“å­˜çš„æ¡†ï¼Œä»… debugModeï¼‰
    if (debugMode && (currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED) && lastBodyRectRef.current) {
      ctx.strokeStyle = "rgba(0, 255, 0, 0.8)";
      ctx.lineWidth = 6 * scale;
      ctx.strokeRect(lastBodyRectRef.current.x, lastBodyRectRef.current.y, lastBodyRectRef.current.width, lastBodyRectRef.current.height);
    }

    // ç»˜åˆ¶æ‰‹éƒ¨ landmarks å’Œ OK æ‰‹åŠ¿ï¼ˆåœ¨æ‰‹åŠ¿æ£€æµ‹çŠ¶æ€ï¼Œä»… debugModeï¼‰
    if (debugMode && handsResults && handsResults.multiHandLandmarks && handsResults.multiHandLandmarks.length > 0 &&
        (currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED)) {
      for (const landmarks of handsResults.multiHandLandmarks) {
        // åªç»˜åˆ¶åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…çš„æ‰‹åŠ¿
        const { width: videoW, height: videoH } = videoDimensionsRef.current;
        if (!isHandInCropArea(landmarks, videoW, videoH)) {
          continue;
        }
        drawConnectors(ctx, landmarks, HAND_CONNECTIONS, { color: "#00FF00", lineWidth: 6 * scale });
        drawLandmarks(ctx, landmarks, { color: "#FF0000", lineWidth: 2 * scale, radius: 8 * scale });

        const gestureResult = recognizeOKGesture(landmarks, {
            circleThreshold: settingsRef.current.gestureCircleThreshold,
            fingerExtendThreshold: settingsRef.current.gestureFingerExtendThreshold,
            confidenceThreshold: settingsRef.current.gestureConfidenceThreshold
        });
        if (gestureResult.isOK) {
          ctx.font = `bold ${Math.round(48 * scale)}px Arial`;
          ctx.fillStyle = "#00FF00";
          ctx.strokeStyle = "#000000";
          ctx.lineWidth = 4 * scale;
          const text = "OK ğŸ‘Œ";
          // æ³¨æ„ï¼štext ä¹Ÿæ˜¯åœ¨ transform ä¸‹ç»˜åˆ¶çš„ã€‚å¦‚æœ text ä½ç½®æ˜¯åŸºäº canvas center è®¡ç®—çš„...
          // width/height æ˜¯ canvas dimensionsã€‚
          // canvas.width åœ¨ transform ä¸‹ä¸å†å¯¹åº” å±å¹•å³è¾¹ç¼˜ã€‚
          // ä½†æ˜¯æˆ‘ä»¬éœ€è¦æ–‡å­—å±…ä¸­...
          // ctx.fillText(text, (canvas.width - textWidth) / 2, 160 * scale);
          // (canvas.width - textWidth) / 2 æ˜¯ canvas åæ ‡ç³»ä¸‹çš„ä¸­å¿ƒã€‚
          // ä½†æ˜¯å½“å‰åæ ‡ç³»è¢«å¹³ç§»äº† (-sx * k)ã€‚
          // å±å¹•ä¸­å¿ƒåœ¨å½“å‰åæ ‡ç³»ä¸‹æ˜¯: (ScreenCenter - T) / S ? No.
          // Drawing at (canvas.width/2) inside transformed context means:
          // VisualPos = T + S * (canvas.width/2).
          // We want VisualPos = CanvasCenter = cWidth/2.
          // So we need to draw at X such that T + S*X = cWidth/2.
          // X = (cWidth/2 - T) / S.
          
          // è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚åŸæœ¬çš„ä»£ç ä½¿ç”¨äº†ç»å¯¹åæ ‡ (canvas.width).
          // å¦‚æœæˆ‘ä»¬ scale/translate äº†æ•´ä¸ª contextï¼Œ
          // ç”¨ "canvas.width" è®¡ç®—å‡ºçš„åæ ‡å°†ä¸å†å¯¹åº”ç‰©ç†ç”»å¸ƒçš„è¾¹ç¼˜ã€‚
          
          // è§£å†³æ–¹æ¡ˆï¼š
          // å¯¹äºâ€œå›ºå®šåœ¨å±å¹•ä½ç½®â€çš„ UIï¼ˆå¦‚å€’è®¡æ—¶ã€çŠ¶æ€æ–‡å­—ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨ restore() ä¹‹åç»˜åˆ¶ï¼
          // å¯¹äºâ€œè·Ÿéšç‰©ä½“â€çš„ UIï¼ˆå¦‚éª¨æ¶ã€æ£€æµ‹æ¡†ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨ restore() ä¹‹å‰ç»˜åˆ¶ã€‚
        }
      }
    }
    
    // æ¢å¤ contextï¼Œä»¥ä¾¿åç»­ç»˜åˆ¶å›ºå®š UI
    ctx.restore();

    // é‡æ–°éå†ç»˜åˆ¶å›ºå®š UI (å€’è®¡æ—¶ï¼ŒçŠ¶æ€æ–‡å­—ï¼Œä»¥åŠ OK æ‰‹åŠ¿çš„æ–‡å­—)
    // OKæ‰‹åŠ¿æ–‡å­—å¦‚æœæ˜¯è·Ÿéšæ‰‹çš„ï¼Œåº”è¯¥åœ¨ä¸Šé¢ç”»ã€‚
    // ä½†åŸä»£ç æ˜¯å›ºå®šåœ¨å±å¹•ä¸Šæ–¹ (160px)ã€‚
    // æ‰€ä»¥ OK æ‰‹åŠ¿æ–‡å­—ä¹Ÿåº”è¯¥ç§»åˆ°å¤–é¢ã€‚

    // è¡¥ç”»: OK æ‰‹åŠ¿æ–‡å­— (å¦‚æœæœ‰)
    if (debugMode && handsResults && handsResults.multiHandLandmarks && handsResults.multiHandLandmarks.length > 0 &&
        (currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED)) {
         let showOK = false;
         for (const landmarks of handsResults.multiHandLandmarks) {
            const { width: videoW, height: videoH } = videoDimensionsRef.current;
            if (isHandInCropArea(landmarks, videoW, videoH) && recognizeOKGesture(landmarks, {
                circleThreshold: settingsRef.current.gestureCircleThreshold,
                fingerExtendThreshold: settingsRef.current.gestureFingerExtendThreshold,
                confidenceThreshold: settingsRef.current.gestureConfidenceThreshold
            }).isOK) {
                showOK = true;
                break;
            }
         }
         if (showOK) {
             ctx.font = `bold ${Math.round(48 * scale)}px Arial`;
             ctx.fillStyle = "#00FF00";
             ctx.strokeStyle = "#000000";
             ctx.lineWidth = 4 * scale;
             const text = "OK ğŸ‘Œ";
             const textWidth = ctx.measureText(text).width;
             ctx.strokeText(text, (canvas.width - textWidth) / 2, 160 * scale);
             ctx.fillText(text, (canvas.width - textWidth) / 2, 160 * scale);
         }
    }

    // ç»˜åˆ¶å€’è®¡æ—¶
    if (currentState === CaptureState.COUNTDOWN) {
      ctx.fillStyle = "#FFD700";
      ctx.strokeStyle = "#000000";
      ctx.lineWidth = 6 * scale;
      ctx.font = `bold ${Math.round(120 * scale)}px Arial`;
      const text = countdownRef.current.toString();
      const textWidth = ctx.measureText(text).width;
      ctx.strokeText(text, (canvas.width - textWidth) / 2, canvas.height / 2);
      ctx.fillText(text, (canvas.width - textWidth) / 2, canvas.height / 2);
    }

    // ç»˜åˆ¶çŠ¶æ€æ¶ˆæ¯ï¼ˆä»… debugMode æˆ–å€’è®¡æ—¶çŠ¶æ€ï¼‰
    const currentStatusMessage = statusMessageRef.current;
    if (debugMode && currentStatusMessage && currentState !== CaptureState.COUNTDOWN) {
      ctx.fillStyle = "#FFFFFF";
      ctx.strokeStyle = "#000000";
      ctx.lineWidth = 4 * scale;
      ctx.font = `bold ${Math.round(32 * scale)}px Arial`;
      const textWidth = ctx.measureText(currentStatusMessage).width;
      ctx.strokeText(currentStatusMessage, (canvas.width - textWidth) / 2, canvas.height - 100 * scale);
      ctx.fillText(currentStatusMessage, (canvas.width - textWidth) / 2, canvas.height - 100 * scale);
    }

    pendingDrawRef.current = false;
  }, [updateFps]);

  // è¯·æ±‚ç»˜åˆ¶
  const requestDraw = React.useCallback(() => {
    if (!pendingDrawRef.current) {
      pendingDrawRef.current = true;
      requestAnimationFrame(draw);
    }
  }, [draw]);

  const onPoseResults = React.useCallback((results: PoseResults) => {
    const currentTime = Date.now();
    const currentState = stateRef.current;
    const canvas = canvasRef.current;
    if (!canvas) return;

    // ä¿å­˜æœ€æ–°çš„ç»“æœ
    latestPoseResultsRef.current = results;

    // å¤„ç†å…¨èº«æ£€æµ‹é€»è¾‘
    if (results.poseLandmarks) {
      // æ£€æŸ¥äººä½“æ˜¯å¦åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…
      const { width: videoW, height: videoH } = videoDimensionsRef.current;
      const isInCropArea = isBodyInCropArea(results.poseLandmarks, videoW, videoH);
      const rect = calculateBodyRect(results.poseLandmarks, canvas.width, canvas.height);
      
      if (rect && isInCropArea) {
        lastBodyDetectedTime.current = currentTime;
        lastBodyRectRef.current = rect;
        lastPoseLandmarksRef.current = results.poseLandmarks;

        if (currentState !== CaptureState.COMPLETED) {
          if (currentState === CaptureState.IDLE) {
            setState(CaptureState.DETECTING_BODY);
            bodyDetectionStartTime.current = currentTime;
            setStatusMessage("æ­£åœ¨æ£€æµ‹å…¨èº«...");
          } else if (currentState === CaptureState.DETECTING_BODY) {
            if (bodyDetectionStartTime.current && currentTime - bodyDetectionStartTime.current >= 1000) {
              setState(CaptureState.DETECTING_GESTURE);
              setStatusMessage("âœ“ å·²è¯†åˆ«åˆ°å…¨èº«ï¼Œè¯·åšå‡ºOKæ‰‹åŠ¿");
            }
          }
        }
      } else if (!isInCropArea && currentState === CaptureState.DETECTING_BODY) {
        // äººä½“ä¸åœ¨è£å‰ªåŒºåŸŸå†…ï¼Œæç¤ºç”¨æˆ·
        setStatusMessage("è¯·ç«™åˆ°ç”»é¢ä¸­å¤®");
      }
    } else {
      // å…¨èº«ä¸¢å¤±å¤„ç†
      if (currentState === CaptureState.DETECTING_BODY) {
        if (lastBodyDetectedTime.current && currentTime - lastBodyDetectedTime.current >= 1000) {
          setState(CaptureState.IDLE);
          bodyDetectionStartTime.current = null;
          lastBodyDetectedTime.current = null;
          lastPoseLandmarksRef.current = null;
          setStatusMessage("âŒ æœªè¯†åˆ«åˆ°å…¨èº«ï¼Œè¯·é‡æ–°ç«™ä½");
          setTimeout(() => setStatusMessage(""), 1500);
        }
      }
    }

    // è¯·æ±‚ç»˜åˆ¶
    requestDraw();
  }, [requestDraw]);

  const onHandsResults = React.useCallback((results: Results) => {
    const currentTime = Date.now();
    const currentState = stateRef.current;
    const video = videoRef.current;
    if (!video) return;

    // ä¿å­˜æœ€æ–°çš„ç»“æœ
    latestHandsResultsRef.current = results;

    if (currentState === CaptureState.DETECTING_GESTURE || currentState === CaptureState.GESTURE_DETECTED) {
      // æ¯5å¸§æ£€æµ‹ä¸€æ¬¡poseï¼Œå‡å°‘æ›´æ–°é¢‘ç‡ï¼Œé¿å…é—ªåŠ¨
      frameCountRef.current++;
      if (frameCountRef.current % 5 === 0 && poseRef.current && video) {
        try {
          poseRef.current.send({ image: video }).catch(() => {});
        } catch (err) {}
      }

      // æ£€æŸ¥å…¨èº«æ˜¯å¦ä¸¢å¤±
      if (lastBodyDetectedTime.current && currentTime - lastBodyDetectedTime.current >= 1000) {
        setState(CaptureState.IDLE);
        bodyDetectionStartTime.current = null;
        lastBodyDetectedTime.current = null;
        gestureDetectionStartTime.current = null;
        frameCountRef.current = 0;
        setStatusMessage("âŒ å…¨èº«ä¸¢å¤±ï¼Œé‡æ–°æ£€æµ‹");
        setTimeout(() => setStatusMessage(""), 1500);
        requestDraw();
        return;
      }

      // æ£€æµ‹ OK æ‰‹åŠ¿ï¼ˆåªæ£€æµ‹åœ¨ 10:16 è£å‰ªåŒºåŸŸå†…çš„æ‰‹åŠ¿ï¼‰
      let isOKDetected = false;
      if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
        for (const landmarks of results.multiHandLandmarks) {
          // å…ˆæ£€æŸ¥æ‰‹åŠ¿æ˜¯å¦åœ¨è£å‰ªåŒºåŸŸå†…
          const { width: videoW, height: videoH } = videoDimensionsRef.current;
          if (!isHandInCropArea(landmarks, videoW, videoH)) {
            continue;
          }
          const gestureResult = recognizeOKGesture(landmarks, {
            circleThreshold: settingsRef.current.gestureCircleThreshold,
            fingerExtendThreshold: settingsRef.current.gestureFingerExtendThreshold,
            confidenceThreshold: settingsRef.current.gestureConfidenceThreshold
          });
          if (gestureResult.isOK) {
            isOKDetected = true;
            break;
          }
        }
      }

      // å¤„ç†æ‰‹åŠ¿çŠ¶æ€
      if (isOKDetected) {
        gestureLastLostTime.current = null;
        
        if (currentState === CaptureState.DETECTING_GESTURE) {
          gestureDetectionStartTime.current = currentTime;
          setState(CaptureState.GESTURE_DETECTED);
        } else if (currentState === CaptureState.GESTURE_DETECTED && gestureDetectionStartTime.current) {
          const elapsed = currentTime - gestureDetectionStartTime.current;
          const remaining = Math.ceil((3000 - elapsed) / 1000);

          if (elapsed >= 3000) {
            setState(CaptureState.COUNTDOWN);
            setCountdown(5);
            setStatusMessage("å‡†å¤‡æ‹ç…§ï¼");
          } else {
            setStatusMessage(`ä¿æŒOKæ‰‹åŠ¿ ${remaining}s`);
          }
        }
      } else {
        // æ‰‹åŠ¿æœªæ£€æµ‹åˆ°
        if (currentState === CaptureState.GESTURE_DETECTED) {
          // åªæœ‰åœ¨æ‰‹åŠ¿æŒç»­ä¸¢å¤±300msåæ‰åˆ‡æ¢çŠ¶æ€ï¼Œé¿å…é—ªåŠ¨
          if (gestureLastLostTime.current === null) {
            gestureLastLostTime.current = currentTime;
          } else if (currentTime - gestureLastLostTime.current >= 300) {
            setState(CaptureState.DETECTING_GESTURE);
            gestureDetectionStartTime.current = null;
            gestureLastLostTime.current = null;
            setStatusMessage("è¯·åšå‡ºOKæ‰‹åŠ¿");
          }
        }
      }

      // è¯·æ±‚ç»˜åˆ¶
      requestDraw();
    }
  }, [requestDraw]);

  React.useEffect(() => {
    if (state === CaptureState.COUNTDOWN) {
      if (countdown > 0) {
        const timer = setTimeout(() => setCountdown(countdown - 1), 1000);
        return () => clearTimeout(timer);
      } else {
        // å€’è®¡æ—¶ç»“æŸï¼Œæ•è·å½“å‰å¸§ä½œä¸ºfrozen frame
        // ä» video å…ƒç´ æ•è·ï¼Œè€Œä¸æ˜¯ canvasï¼Œè¿™æ ·ä¸ä¼šåŒ…å«çº¿æ¡†å’Œå€’è®¡æ—¶
        const video = videoRef.current;
        const canvas = canvasRef.current;
        if (video && canvas) {
          try {
            // åˆ›å»ºä¸´æ—¶ canvas ç”¨äºæ•è·å’Œè£å‰ª
            const tempCanvas = document.createElement('canvas');
            const sourceWidth = video.videoWidth || 640;
            const sourceHeight = video.videoHeight || 480;
            
            // è®¡ç®— 10:16 è£å‰ªåŒºåŸŸï¼ˆä¸æ˜¾ç¤ºåŒºåŸŸä¿æŒä¸€è‡´ï¼‰
            const targetAspect = TARGET_ASPECT_RATIO; // 10 / 16
            const sourceAspect = sourceWidth / sourceHeight;
            
            let cropWidth, cropHeight, cropX, cropY;
            if (sourceAspect > targetAspect) {
              // Source is wider, crop the width
              cropHeight = sourceHeight;
              cropWidth = sourceHeight * targetAspect;
              cropX = (sourceWidth - cropWidth) / 2;
              cropY = 0;
            } else {
              // Source is taller, crop the height
              cropWidth = sourceWidth;
              cropHeight = sourceWidth / targetAspect;
              cropX = 0;
              cropY = (sourceHeight - cropHeight) / 2;
            }
            
            // è®¾ç½®ä¸´æ—¶ canvas ä¸ºè£å‰ªåçš„å°ºå¯¸
            tempCanvas.width = cropWidth;
            tempCanvas.height = cropHeight;
            
            const tempCtx = tempCanvas.getContext('2d');
            if (tempCtx) {
              // ä» video ç»˜åˆ¶è£å‰ªåçš„åŒºåŸŸåˆ°ä¸´æ—¶ canvas
              tempCtx.drawImage(
                video,
                cropX, cropY, cropWidth, cropHeight,
                0, 0, cropWidth, cropHeight
              );
              
              // è½¬æ¢ä¸º data URL
              const frozenFrameUrl = tempCanvas.toDataURL('image/png');
              frozenFrameRef.current = frozenFrameUrl;
              console.log('âœ… Frozen frame captured at countdown end (clean, 10:16 cropped)');
            }
          } catch (err) {
            console.error('âŒ Failed to capture frozen frame:', err);
          }
        }
        // ç«‹å³è¿›å…¥æ‹ç…§ä¸­çŠ¶æ€æ˜¾ç¤ºå¿«é—¨æ•ˆæœ
        setState(CaptureState.CAPTURING);
      }
    }
  }, [state, countdown]);

  // Handle capturing state - immediately take photo (no delay)
  React.useEffect(() => {
    if (state === CaptureState.CAPTURING) {
      // ç›´æ¥è¿›å…¥ CAPTURE çŠ¶æ€ï¼Œä¸å†ç­‰å¾… 3 ç§’
      setState(CaptureState.CAPTURE);
    }
  }, [state]);

  // Handle photo capture - output 10:16 aspect ratio image
  React.useEffect(() => {
    if (state === CaptureState.CAPTURE) {
      const video = videoRef.current;
      if (video) {
        const sourceWidth = video.videoWidth || 640;
        const sourceHeight = video.videoHeight || 480;
        
        // è®¡ç®— 10:16 è£å‰ªåŒºåŸŸ
        const targetAspect = TARGET_ASPECT_RATIO; // 10 / 16 = 0.625
        const sourceAspect = sourceWidth / sourceHeight;
        
        let cropWidth, cropHeight, cropX, cropY;
        if (sourceAspect > targetAspect) {
          // è§†é¢‘æ›´å®½ï¼Œéœ€è¦è£å‰ªå®½åº¦
          cropHeight = sourceHeight;
          cropWidth = sourceHeight * targetAspect;
          cropX = (sourceWidth - cropWidth) / 2;
          cropY = 0;
        } else {
          // è§†é¢‘æ›´é«˜ï¼Œéœ€è¦è£å‰ªé«˜åº¦
          cropWidth = sourceWidth;
          cropHeight = sourceWidth / targetAspect;
          cropX = 0;
          cropY = (sourceHeight - cropHeight) / 2;
        }
        
        // åˆ›å»º 10:16 æ¯”ä¾‹çš„ç”»å¸ƒ
        const resultCanvas = document.createElement("canvas");
        resultCanvas.width = cropWidth;
        resultCanvas.height = cropHeight;
        
        const resultCtx = resultCanvas.getContext("2d");
        if (resultCtx) {
          // ä»è§†é¢‘ä¸­è£å‰ª 10:16 åŒºåŸŸ
          resultCtx.drawImage(
            video,
            cropX, cropY, cropWidth, cropHeight,
            0, 0, cropWidth, cropHeight
          );
          
          console.log(`æˆªå›¾å®Œæˆ: ${cropWidth}x${cropHeight} (æ¯”ä¾‹ ${(cropWidth/cropHeight).toFixed(3)})`);
          onCapture?.(resultCanvas);
        }
        
        setState(CaptureState.COMPLETED);
        setStatusMessage("âœ“ æ‹ç…§å®Œæˆï¼");
      }
    }
  }, [state, onCapture]);

  // Track when video element is ready
  const [videoReady, setVideoReady] = React.useState(false);
  
  // Callback ref for video element
  const videoCallbackRef = React.useCallback((node: HTMLVideoElement | null) => {
    videoRef.current = node;
    if (node) {
      console.log('âœ… Video å…ƒç´ å·²é™„åŠ åˆ° DOM');
      setVideoReady(true);
    }
  }, []);

  // Initialization effect - runs when video is ready
  React.useEffect(() => {
    if (!videoReady) {
      console.log('ç­‰å¾… video å…ƒç´ ...');
      return;
    }

    let mounted = true;
    let camera: Camera | null = null;
    let hands: Hands | null = null;
    let pose: Pose | null = null;

    const initialize = async () => {
      try {
        console.log('=======å¼€å§‹åˆå§‹åŒ–=======');
        console.log('videoRef.current:', videoRef.current);
        console.log('canvasRef.current:', canvasRef.current);
        
        if (!videoRef.current || !canvasRef.current) {
          throw new Error('Video æˆ– Canvas å…ƒç´ æœªæ‰¾åˆ°');
        }

        // Initialize sequentially to avoid race conditions with shared global variables in loaders
        console.log('Initializing Pose...');
        const poseInstance = await initializePose();
        
        console.log('Initializing Hands...');
        const handsInstance = await (async () => {
          const h = new Hands({ 
            locateFile: (file) => {
              if (file.includes('pose')) {
                return `${import.meta.env.BASE_URL}mediapipe/pose/${file}`;
              }
              return `${import.meta.env.BASE_URL}mediapipe/hands/${file}`;
            }
          });
          h.setOptions({ 
            maxNumHands: settingsRef.current.handsMaxNum, 
            modelComplexity: settingsRef.current.handsModelComplexity, 
            minDetectionConfidence: settingsRef.current.handsMinDetectionConfidence, 
            minTrackingConfidence: settingsRef.current.handsMinTrackingConfidence 
          });
          return h;
        })();

        if (!mounted) return;
        pose = poseInstance;
        pose.onResults(onPoseResults);
        poseRef.current = pose;
        hands = handsInstance;
        hands.onResults(onHandsResults);
        handsRef.current = hands;

        console.log('åˆ›å»º Camera å®ä¾‹...');
        
        // ä»è®¾ç½®ä¸­è·å–åˆ†è¾¨ç‡
        const targetWidth = settingsRef.current.videoWidth;
        const targetHeight = settingsRef.current.videoHeight;
        console.log(`ğŸ“¹ è¯·æ±‚åˆ†è¾¨ç‡: ${targetWidth}x${targetHeight}`);
        
        camera = new Camera(videoRef.current, {
          onFrame: async () => {
            if (!mounted || !videoRef.current) return;
            const currentState = stateRef.current;
            
            try {
              // åœ¨æ‰‹åŠ¿æ£€æµ‹é˜¶æ®µåŒæ—¶è¿è¡Œ pose å’Œ hands
              if (
                currentState === CaptureState.DETECTING_GESTURE ||
                currentState === CaptureState.GESTURE_DETECTED
              ) {
                // ä¸ç­‰å¾… pose å®Œæˆï¼Œè®©å®ƒåœ¨åå°è¿è¡Œ
                if (pose && mounted) {
                  pose.send({ image: videoRef.current }).catch(() => {});
                }
                // hands æ˜¯ä¸»è¦å¤„ç†ï¼Œç­‰å¾…å®ƒå®Œæˆ
                if (hands && mounted) {
                  await hands.send({ image: videoRef.current });
                }
              } else {
                // å…¶ä»–çŠ¶æ€åªè¿è¡Œ pose
                if (pose && mounted) {
                  await pose.send({ image: videoRef.current });
                }
              }
            } catch (err) {
              // å¿½ç•¥é”™è¯¯ï¼Œç»§ç»­ä¸‹ä¸€å¸§
            }
          },
          width: targetWidth,
          height: targetHeight,
        });

        console.log('æ­£åœ¨å¯åŠ¨æ‘„åƒå¤´...');
        await camera.start();
        console.log('âœ… æ‘„åƒå¤´å¯åŠ¨æˆåŠŸï¼');
        
        // æ›´æ–°å®é™…è§†é¢‘åˆ†è¾¨ç‡
        if (videoRef.current) {
          const requestedWidth = settingsRef.current.videoWidth;
          const requestedHeight = settingsRef.current.videoHeight;
          const actualWidth = videoRef.current.videoWidth || targetWidth;
          const actualHeight = videoRef.current.videoHeight || targetHeight;
          videoDimensionsRef.current = { width: actualWidth, height: actualHeight };
          
          if (actualWidth !== requestedWidth || actualHeight !== requestedHeight) {
            console.log(`ğŸ“¹ è¯·æ±‚åˆ†è¾¨ç‡: ${requestedWidth}x${requestedHeight}ï¼Œæ‘„åƒå¤´å®é™…è¾“å‡º: ${actualWidth}x${actualHeight}`);
          } else {
            console.log(`âœ… è§†é¢‘åˆ†è¾¨ç‡: ${actualWidth}x${actualHeight}`);
          }
        }
        
        cameraRef.current = camera;
        
        // é¢„çƒ­ hands æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡ä½¿ç”¨æ—¶å¡é¡¿
        console.log('é¢„çƒ­ hands æ¨¡å‹...');
        if (videoRef.current && hands) {
          try {
            await hands.send({ image: videoRef.current });
            console.log('âœ… hands æ¨¡å‹é¢„çƒ­å®Œæˆ');
          } catch (err) {
            console.log('é¢„çƒ­å¤±è´¥ï¼Œä½†ä¸å½±å“ä½¿ç”¨');
          }
        }
        
        setIsLoading(false);
        setState(CaptureState.IDLE);
        setStatusMessage("ç«™åœ¨æ‘„åƒå¤´å‰å¼€å§‹æ£€æµ‹");
        console.log('âœ… åˆå§‹åŒ–å®Œæˆ');
      } catch (err) {
        console.error('âŒ åˆå§‹åŒ–å¤±è´¥:', err);
        if (mounted) {
          setError(`æ— æ³•å¯åŠ¨æ‘„åƒå¤´æˆ–åŠ è½½æ¨¡å‹: ${err instanceof Error ? err.message : String(err)}`);
          setIsLoading(false);
        }
      }
    };

    initialize();

    return () => {
      mounted = false;
      if (camera) camera.stop();
      setTimeout(() => {
        if (hands) hands.close();
        if (pose) pose.close();
      }, 100);
    };
  }, [videoReady, onHandsResults, onPoseResults]);

  
  // åœæ­¢æ‘„åƒå¤´ï¼ˆè¿›å…¥ Page5 æ—¶è°ƒç”¨ï¼Œé¿å…å¤šæ‘„åƒå¤´å†²çªï¼‰
  const stopCamera = React.useCallback(() => {
    if (cameraRef.current) {
      console.log('æ­£åœ¨åœæ­¢ä¸»æ‘„åƒå¤´...');
      cameraRef.current.stop();
      cameraRef.current = null;
      console.log('âœ… ä¸»æ‘„åƒå¤´å·²åœæ­¢');
    }
  }, []);

  // é‡æ–°å¯åŠ¨æ‘„åƒå¤´ï¼ˆä» Page5 è¿”å›æ—¶è°ƒç”¨ï¼‰
  const restartCamera = React.useCallback(async () => {
    if (!videoRef.current) {
      console.log('æ— æ³•é‡å¯æ‘„åƒå¤´: video å…ƒç´ ä¸å­˜åœ¨');
      return;
    }
    
    if (cameraRef.current) {
      console.log('æ‘„åƒå¤´å·²åœ¨è¿è¡Œ');
      return;
    }

    try {
      console.log('æ­£åœ¨é‡æ–°å¯åŠ¨æ‘„åƒå¤´...');
      const video = videoRef.current;
      
      const camera = new Camera(video, {
        onFrame: async () => {
          if (!videoRef.current) return;
          const currentState = stateRef.current;
          
          try {
            if (
              currentState === CaptureState.DETECTING_GESTURE ||
              currentState === CaptureState.GESTURE_DETECTED
            ) {
              if (poseRef.current) {
                poseRef.current.send({ image: videoRef.current }).catch(() => {});
              }
              if (handsRef.current) {
                await handsRef.current.send({ image: videoRef.current });
              }
            } else {
              if (poseRef.current) {
                await poseRef.current.send({ image: videoRef.current });
              }
            }
          } catch (err) {
            // å¿½ç•¥é”™è¯¯
          }
        },
        width: settingsRef.current.videoWidth,
        height: settingsRef.current.videoHeight,
      });

      await camera.start();
      cameraRef.current = camera;
      
      // ç­‰å¾… video å…ƒç´ æ­£ç¡®åŠ è½½åˆ†è¾¨ç‡ï¼ˆæœ€å¤šç­‰å¾… 2 ç§’ï¼‰
      const waitForVideoDimensions = (): Promise<{ width: number; height: number }> => {
        return new Promise((resolve) => {
          let attempts = 0;
          const maxAttempts = 40; // 40 * 50ms = 2s
          
          const checkDimensions = () => {
            attempts++;
            if (video.videoWidth > 0 && video.videoHeight > 0) {
              console.log(`âœ… è§†é¢‘åˆ†è¾¨ç‡å·²å°±ç»ª: ${video.videoWidth}x${video.videoHeight} (å°è¯• ${attempts} æ¬¡)`);
              resolve({ width: video.videoWidth, height: video.videoHeight });
            } else if (attempts >= maxAttempts) {
              console.warn(`âš ï¸ ç­‰å¾…è§†é¢‘åˆ†è¾¨ç‡è¶…æ—¶ï¼Œä½¿ç”¨è®¾ç½®å€¼ ${settingsRef.current.videoWidth}x${settingsRef.current.videoHeight}`);
              resolve({ width: settingsRef.current.videoWidth, height: settingsRef.current.videoHeight });
            } else {
              setTimeout(checkDimensions, 50);
            }
          };
          
          checkDimensions();
        });
      };
      
      const dimensions = await waitForVideoDimensions();
      videoDimensionsRef.current = dimensions;
      
      console.log('âœ… æ‘„åƒå¤´é‡æ–°å¯åŠ¨æˆåŠŸ');
    } catch (err) {
      console.error('é‡å¯æ‘„åƒå¤´å¤±è´¥:', err);
    }
  }, []);

  const handleReset = React.useCallback(() => {
    setState(CaptureState.IDLE);
    setStatusMessage("");
    setCountdown(5);
    bodyDetectionStartTime.current = null;
    lastBodyDetectedTime.current = null;
    gestureDetectionStartTime.current = null;
    gestureLastLostTime.current = null;
    lastBodyRectRef.current = null;
    lastPoseLandmarksRef.current = null;
    frameCountRef.current = 0;
    frozenFrameRef.current = null;
    
    // å»¶è¿Ÿé‡å¯æ‘„åƒå¤´ï¼Œç­‰å¾… Page5 çš„æ‘„åƒå¤´å®Œå…¨é‡Šæ”¾ï¼ˆé¿å…æ‘„åƒå¤´èµ„æºå†²çªï¼‰
    setTimeout(() => {
      restartCamera();
    }, 300);
  }, [restartCamera]);

  return {
    videoRef: videoCallbackRef,
    canvasRef,
    state,
    isLoading,
    error,
    statusMessage,
    countdown,
    handleReset,
    stopCamera,
    frozenFrame: frozenFrameRef.current,
  };
}
